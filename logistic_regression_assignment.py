# -*- coding: utf-8 -*-
"""LOGISTIC_REGRESSION_assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lz1n5XuBhRjFRbTXc6tIN2NbNjI9TlkE
"""

# Import necessary libraries for data manipulation, visualization, and machine learning
import pandas as pd

 # Load the training dataset from a CSV file
 # Display the first few rows of the training dataset
 #Perform EDA process
test=pd.read_csv('Titanic_test.csv')
test.head()

# Load the test dataset from a CSV file
# Display the first few rows of the test dataset
train=pd.read_csv('Titanic_train.csv')
train.head()

print("train shape:   ",train.shape)
print("test shape:    ",test.shape)

train.columns

test.columns

train.info()

test.info()

train.describe()

test.describe()

train.isnull().sum()

test.isnull().sum()

#Create visualizations such as histograms, box plots, or pair plots
#to visualize the distributions and relationships between features.
#Analyze any patterns or correlations observed in the data.
import seaborn as sns
import matplotlib.pyplot as plt

corr_matrix_train=train.corr(numeric_only=True)
corr_matrix_train
sns.heatmap(corr_matrix_train, annot=True)
plt.show()

corr_matrix_test=test.corr(numeric_only=True)
corr_matrix_test
sns.heatmap(corr_matrix_test, annot=True, cmap='Blues')
plt.show()

ax = train["Age"].hist(bins=15, density=True, stacked=False, color='teal', alpha=0.6)
train["Age"].plot(kind='density', color='teal')
ax.set(xlabel='Age')
plt.xlim(-10,85)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

sns.histplot(train['Age'])
plt.title='Age Distribution'
plt.show()

sns.histplot(train['Fare'])
plt.title='Fare Distribution'
plt.show()

sns.boxplot(x='Survived', y='Age', data=train)
plt.title('Age Distribution by Survival')
plt.show()

sns.plot(x='Survived', y='Fare', data=train)
plt.title('Fare Distribution by Survival')
plt.show()

train['Age']=train['Age'].fillna(train['Age'].median())
test['Age']=test['Age'].fillna(test['Age'].median())

train['Cabin']=train['Cabin'].fillna(train['Cabin'].mode()[0])
test['Cabin']=test['Cabin'].fillna(test['Cabin'].mode()[0])

train['Embarked']=train['Embarked'].fillna(train['Embarked'].mode()[0])

test['Fare']=test['Fare'].fillna(test['Fare'].median())

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
for col in ['Sex', 'Embarked', 'Cabin']:
    train[col]=le.fit_transform(train[col])
    test[col]=le.fit_transform(test[col])
#train['Sex', 'Embarked']=le.fit_transform(train['Sex', 'Embarked'])
#test['Sex', 'Embarked']=le.fit_transform(test['Sex', 'Embarked'])

train

test

features=[  'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']
X=train[features]
y=train['Survived']
X_test=test[features]

from sklearn.linear_model import LogisticRegression
model=LogisticRegression()
model.fit(X,y)
y_pred=model.predict(X_test)
y_pred

from sklearn.metrics import accuracy_score
accuracy_score(y,model.predict(X))

from sklearn.metrics import classification_report
print(classification_report(y,model.predict(X)))

"""Interpretation
The metrics indicate that the model performs well, with:

- An overall accuracy of 80%
- A macro average F1-score of 0.79, indicating a good balance between precision and recall for both classes
- A weighted average F1-score of 0.80, which takes into account the class imbalance

The model seems to perform slightly better on Class 0, with higher precision, recall, and F1-score compared to Class 1. However, the performance on Class 1 is still reasonable, with a precision of 0.78 and an F1-score of 0.73.
"""

from sklearn.metrics import confusion_matrix
confusion_matrix(y,model.predict(X))

"""True positive= 483
True negatives=66
False positive=108
False negative=234
"""

from sklearn.metrics import classification_report
print(classification_report(y,model.predict(X)))

from sklearn.metrics import roc_auc_score
roc_auc_score(y,model.predict(X))

from sklearn.metrics import roc_curve
roc_curve(y,model.predict(X))

fpr, tpr, thresholds = roc_curve(y, model.predict(X))

roc_auc = roc_auc_score(y, model.predict(X))

# prompt: visualize the roc curve

plt.figure()
plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.show()

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

param_grid = {
    'C': [0.1, 1, 10],
    'penalty': ['l1', 'l2'],
    'max_iter': [1000, 2000]
}

grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)
grid_search.fit(X, y)

print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)

best_model = grid_search.best_estimator_

"""The significance of features in predicting the target variable can be evaluated based on their coefficients, p-values, and confidence intervals.

- Features with large coefficients: Are likely to be important predictors of survival probability.
- Features with small p-values: Are statistically significant and likely to be important predictors of survival probability.

In the context of the Titanic dataset, features such as:

- Sex: Is likely to be a significant predictor of survival probability, with females having a higher chance of survival.
- Pclass: May also be a significant predictor, with passengers in higher classes having a higher chance of survival.
- Age: Could be a significant predictor, with younger passengers having a higher chance of survival.

By evaluating the coefficients and significance of features, we can gain insights into the relationships between the features and the target variable, and identify the most important predictors of survival probability.

#Conclusion
The logistic regression model predicted survival on the Titanic with 80% accuracy. Key factors like sex, class, and age were important in determining survival chances. While the model performed well, there's room for improvement, especially in reducing errors. Overall, the analysis shows the potential of logistic regression in predicting survival outcomes.
"""

import streamlit as st
import pickle

# prompt: help me deploy this file on streamlit

4.  **Modify the code for Streamlit:** The current code includes a lot of analysis and plotting directly. For a Streamlit app, you want to present this in a user-friendly way. Here's a modified version of your code that can be used as a Streamlit app.
5.  **Run the Streamlit app:** Open your terminal or command prompt, navigate to the directory where you saved `titanic_app.py`, and run the following command: